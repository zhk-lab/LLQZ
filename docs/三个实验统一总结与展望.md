# 三个实验统一总结与展望（实验一/二/三）

> 本文用于论文/答辩的“统一总结与展望”章节：串联三项实验的共同目标、关键发现与方法贡献，并给出不足与未来发展方向。  
> 相关材料：  
> - 实验一：`docs/实验一结果分析报告.md`，`runs/exp1/metrics_summary.json`，`runs/exp1/metrics_k3_all_strategies.json`  
> - 实验二：`docs/实验二结果分析报告.md`，`runs/exp2/correlation_v2.json`  
> - 实验三：`docs/实验三结果分析报告.md`，`runs/exp3_v2/metrics_summary.json`

---

## 1. 总体目标：面向垂直领域幻觉抑制的 Dynamic RAG 可靠性链路

本课题面向垂直领域大语言模型普遍存在的“幻觉/事实不一致”问题，围绕 **Dynamic RAG** 的核心思想展开：先对生成内容的**语义不确定性**进行量化（可信度建模），再据此进行**自适应检索增强**，并通过**自反思机制**对低置信度内容进行校验、修正，必要时“少说/不说”（截断/保守输出），以提升整体可靠性。

三个实验对应 Dynamic RAG 的三个关键组件，并形成逐层增强的验证链路：

1. **实验一（Retrieval）**：提升可用证据的“前列质量”，为生成提供更好的事实支撑。
2. **实验二（Uncertainty）**：用语义熵 \(SE(x)\) 作为“风险/易错”信号，实现可解释的不确定性预警。
3. **实验三（Correction）**：在固定检索证据下，通过 Self-RAG 的候选生成与反思选择，提升回答质量并实现净纠错增益。

这对应一个可落地的系统范式：**先找对证据 → 再识别高风险问题 → 最后触发更强的纠错/反思策略**。

---

## 2. 关键结论（跨实验）

### 2.1 实验一：重排序的价值在“小上下文”更显著

在 BioASQ 上，`hybrid_rerank` 的主要收益来自**把正确证据前置**。当评估口径与下游使用对齐（如 `top_k=3`）时，优势更清晰：

- 相比 `hybrid`，`hybrid_rerank` 在 `K=3` 下的 `MRR@nDCG` 均有明显提升（见 `runs/exp1/metrics_k3_all_strategies.json` 与 `docs/实验一结果分析报告.md`）。
- 代价是更高端到端时延，体现“质量—成本”的可解释权衡。

**统一解读**：若系统只会把前 3–5 条证据喂给模型（常见于 RAG），则应优先报告/优化小 K 的排序指标，否则会被 `Recall@20` 的“命中饱和”掩盖真实收益。

### 2.2 实验二：语义熵可作“风险预警信号”，但效应量中等

实验二在 PubMedQA 上验证：基于**双向蕴含近似 + 贪婪聚类**计算的 \(SE(x)\) 与错误呈统计显著正相关（Spearman 约 0.17，p-value \(\ll 0.001\)，详见 `docs/实验二结果分析报告.md`）。

**统一解读**：语义熵适合作为“触发器/组合信号”，用于决定何时启用更昂贵的纠错机制（如实验三），而不是单独作为强判别器。

### 2.3 实验三：Self-RAG 在固定检索证据下实现显著净纠错

在不重跑实验一、复用 `hybrid_rerank` 检索证据的前提下，Self-RAG（full）实现了：

- Correct Rate 明显提升（+8.5 个百分点）
- Fix（Wrong→Correct）多于 Introduce（Correct→Wrong）
- 配对检验 p-value < 0.01（统计显著）

详见 `docs/实验三结果分析报告.md` 与 `runs/exp3_v2/metrics_summary.json`。

**统一解读**：当检索证据质量可用时，Self-RAG 的收益主要来自“候选差异 + 反思选择 + 保守门控”降低引入新错，并在整体上实现净纠错。

---

## 3. 方法与工程贡献（可作为论文贡献点）

本工作贡献分为四个层面（宏观 → 系统 → 模块 → 局部实现），便于在论文中“从大到小”陈述。

### 3.1 宏观层面贡献（框架与研究问题分解）

- **贡献 1：面向幻觉抑制的 Dynamic RAG 分解框架**  
  将“减少垂直领域幻觉/事实不一致”这一系统目标分解为三类可验证机制：  
  \[
  \text{证据前列排序（检索质量）} \rightarrow \text{语义不确定性量化（风险信号）} \rightarrow \text{自反思选择/纠错（纠错机制）}
  \]
  并用实验一/二/三分别对三类机制给出实证支撑，从而形成可用于论文叙事的闭环链路（不是孤立的三组实验）。

- **贡献 2：把“可信度”从抽象概念落到可计算信号**  
  采用语义熵 \(SE(x)\) 作为对同一问题多次采样后“语义分裂程度”的度量，用于刻画回答可信度与风险水平，为后续“是否/如何增强检索与反思”提供决策依据（实验二）。

- **贡献 3：提出“与下游使用对齐”的评估原则并给出案例**  
  证明当下游仅使用前 3–5 条证据（常见 RAG 设置）时，评价应优先报告小 K（如 `top_k=3`）的排序指标；在 `K=20` 的饱和命中口径下，重排序的真实价值会被掩盖（实验一对比 `metrics_k3_all_strategies.json` vs `metrics_summary.json`）。

### 3.2 系统层面（端到端实验平台与可复现性）

- **端到端流水线与版本化产物**：三项实验均输出稳定的 JSON/JSONL 产物与单图可视化，便于复现实验与论文作图（见 `docs/实验一结果分析报告.md`、`docs/实验二结果分析报告.md`、`docs/实验三结果分析报告.md`）。
- **长任务工程可用性**：实验三实现了并发执行、checkpoint 落盘与 `--resume` 续跑能力，使“意外中断/隔夜运行”具备工程鲁棒性（产物落在 `runs/exp3_v2/`）。

### 3.3 模块层面（核心算法/机制的可验证增益）

- **实验一：重排序的“小 K 优势”被定量放大**（BioASQ）  
  在 `K=3` 口径下，`hybrid_rerank` 相比 `hybrid` 的提升更直观：`MRR@3 +0.0621`、`nDCG@3 +0.0828`（见 `runs/exp1/metrics_k3_all_strategies.json`）。
- **实验二：语义熵的统计显著相关性**（PubMedQA）  
  v2 结果显示 \(SE(x)\) 与错误（binary/ordinal）相关显著（Spearman 约 0.17，p-value \(\ll 0.001\)；详见 `docs/实验二结果分析报告.md`）。
- **实验三：Self-RAG 的显著净纠错**（BioASQ，复用实验一检索证据）  
  `self_rag_full` 的 Correct Rate 从 `0.535` 提升到 `0.620`（`+0.085`），配对检验 `p=0.00599`（见 `runs/exp3_v2/metrics_summary.json` 与 `docs/实验三结果分析报告.md`）。

### 3.4 局部实现层面（关键技术细节）

为贴合 Dynamic RAG 的工程目标（“低置信度时自动修正与截断，避免继续瞎编”），本工作在实现层面强化了可复现与鲁棒性，主要体现在：

- **贡献 4：面向长跑实验的可恢复与版本化产物管理**  
  在实验三实现 checkpoint 落盘与 `--resume` 续跑，避免中断导致整批结果丢失；输出按 `out_root` 组织，便于论文版本对比与追踪（最新版结果以 `runs/exp3_v2/` 为准）。

- **贡献 5：反思信号的可解释化与“净纠错优先”的保守选择**  
  将自反思拆解为 `rel/sup/use` 三类可解释信号，并在 `self_rag_full` 中引入阈值门控、引用加分与不安全惩罚，使选择策略更偏向“减少 Correct→Wrong（引入新错）”，与“低置信度内容应修正或少说”的课题目标一致（详见 `docs/实验三结果分析报告.md` 的设置与纠错统计）。

- **贡献 6：把“是否显著”落到统计检验而非主观观感**  
  对实验三报告 fix/introduce 的结构性统计，并给出配对显著性检验（`paired_sign_pvalue`），使“纠错增益是否显著”具备可复核的统计依据（见 `runs/exp3_v2/metrics_summary.json`）。

---

## 4. 不足与威胁（Limitations）

不足点尽量具体到“会影响哪个指标/哪个结论”，以便在论文 Discussion 中对应回应。

- **评价函数偏弱语义**：实验一/三的回答质量以 token-F1 为主（FactScore proxy）。医学 QA 中同义改写常见，可能导致“质量提升但分数涨幅不成比例”，从而低估真实增益或削弱可解释性。
- **证据标签弱监督噪声**：BioASQ 的 `gold_docs` 与 `ideal_answer` 并非严格“可验证证据—断言”对齐标注，Citation Precision 与 Correct Rate 仍是近似指标，存在上界偏差。
- **judge/critic 上限与自评偏差**：实验二的 NLI 判定与实验三的 critic 在本地小模型条件下存在能力上限；尤其 critic 与生成同模时，可能出现“自洽但不纠错”，或输出格式不稳定导致打分退化为默认值。
- **上下文窗口与证据长度约束**：当 `top_n` 较大、chunk 文本较长时会触发 context 超限，造成失败重试与吞吐下降，影响长跑稳定性。
- **成本与吞吐权衡未系统量化**：目前更多展示“效果 vs 成本”的静态对比；对工程落地而言，还需要给出在固定预算/固定时延约束下的最优策略选择。

---

## 5. 未来发展方向（Outlook）

未来工作按“最能提升论文说服力”和“最能提升系统可用性”两条线并行推进。

### 5.1 更强、更稳的评估与 judge（提升论文说服力）

- **替换/补充 token-F1**：在实验一/三引入 BERTScore 或 LLM-as-judge（给出一致性与抽样人工核验），并报告与 token-F1 的一致性/差异来源，从而更公平地衡量同义改写与事实等价。
- **critic 与生成解耦**：为实验三引入“独立 judge”（不同模型/更强模型/更稳的 JSON mode），重点目标是降低 `introduce_rate`，并在报告中明确展示 fix/introduce 的变化趋势。

### 5.2 从“触发式系统”走向闭环：SE 驱动 Self-RAG

- **SE 作为触发器（成本可控）**：仅对高熵样本启用 Self-RAG 的多候选+反思，低熵样本走 naive，从而在固定预算下最大化整体 Correct Rate。
- **画出触发曲线**：按熵阈值分段报告“整体增益 vs 额外时延/额外调用次数”，把实验二与实验三真正闭环成系统级结论。

### 5.3 更严格的语义熵与语义等价判定

- **logprob 严谨化**：若后端支持 token logprob，用更严格的 \(p(c_k|x)\) 估计替代“簇频率近似”，并报告对相关性与稳定性的影响。
- **专用 NLI/跨编码器替代**：用专用 NLI 模型或 cross-encoder entailment 取代 LLM 判定，并增加小规模人工对照集，报告判定一致性与误差来源。

### 5.4 检索侧的进一步优化与消融

- **面向小上下文优化 rerank**：围绕 `Gold-in-Context@N`（N=3/5）做目标优化与消融，直接对齐“生成可见证据”。
- **更细的证据覆盖诊断**：报告 Evidence coverage（前 N 条证据覆盖 gold_docs 的比例）与排序增益分解（sparse/dense/hybrid/rerank），强化实验一到实验三的因果链条。

### 5.5 稳健性与可用性

- **自动降级策略**：遇到 context 超限时自动截断 evidence（或降 `top_n`/压缩证据），并记录降级次数与对质量的影响，避免长跑中断。
- **更细粒度 checkpoint**：将 checkpoint 从“每 N 题”扩展到“分阶段/分系统落盘”，并统一 out_root 命名（如 `exp3_v3`），保证多轮对比不覆盖且可追踪。

---

## 6. 总结（可用于论文最后一段）

本文通过三个互补实验，从检索排序、语义不确定性到自反思纠错，构建了一个面向生物医学问答可靠性的可解释实验框架：实验一证明重排序在小上下文证据设置下具有明确收益；实验二验证语义熵能作为统计显著的风险预警信号；实验三表明在固定检索证据下，Self-RAG 可实现显著净纠错并提升引用可靠性。未来工作将进一步引入更强 judge、更语义的评价体系，并把语义熵与 Self-RAG 结合为触发式闭环系统，以在可控成本下最大化可靠性增益。

