# 实验与数据集匹配指南（对应 `实验设计整理.md` 的三个实验）

本文件整理了 `rag场景与数据集.pdf` 中给出的数据集。

---

## 可执行实验 Plan（按步骤照做）

下面给你一份**可以照着一步步做**的实验执行计划（对应 `实验设计整理.md` 的实验一/二/三）。

- **你当前选择的数据集（按本文推荐）**
  - 实验一：**BioASQ-QA**
  - 实验二：**PubMedQA**
  - 实验三：**BioASQ-QA**

> 如果你改走法律路线（CUAD / CaseHOLD），整体步骤完全一样；你只需要把“语料库怎么建、指标怎么判定”那两处替换掉即可。

### 0) 通用准备（建议先做完再跑三个实验）

#### 0.1 建议的文件夹结构（为了不乱）
- `data/`：原始数据、清洗后的数据、划分后的 train/dev/test
- `corpus/`：可检索语料库（chunk 后的文档）
- `indexes/`：BM25 索引、向量索引、rerank 模型缓存（如有）
- `runs/`：每次实验的输出（检索结果、回答、引用、日志、指标）
- `plots/`：相关散点图/分桶图、柱状图、成本-效果曲线（如有 ROC/PR 也放这里）

#### 0.2 固定“对比必须一致”的参数（写进实验设置，保证公平）
- **Chunk**：chunk size（300 tokens）与 overlap（30 tokens）固定
- **检索**：Top-K（召回数，K=20）固定；Top-N（注入上下文数，N=3）固定
- **生成**：同一 LLM、同一提示词模板、同一解码参数（temperature/top-p/max_tokens）固定
- **实验二采样**：固定 **\(M=10\)**；后续可再做 5/10/20 的敏感性（可选）

#### 0.3 统一“引用输出格式”（否则实验三很难评）
让模型在回答中对关键断言给引用：
- 在句末加 `[doc_id:chunk_id]`



<!-- ## 实验一：动态检索策略有效性（Sparse vs Dense vs Hybrid vs Rerank）

### 能用哪些数据集（推荐优先级）
- **强推荐**：**BioASQ-QA**（多文档证据天然适配）

### 分别怎么用（通俗步骤）
以 **BioASQ-QA**为例（最像“真正的 RAG 实验”）：

1. **先建一个“可检索语料库”**
   - BioASQ：把医学文献摘要/题目相关材料切分成 chunk 建索引。

2. **同一批问题，分别跑 4 种检索策略**
   - Sparse Only：BM25
   - Dense Only：向量检索
   - Hybrid：BM25 + 向量加权融合
   - Hybrid + Rerank：Hybrid 召回后，再用 Cross-Encoder 重排序

3. **把 Top-N 文档作为上下文，喂给同一个生成模型回答**
   - 关键是：**生成模型、提示词、Top-K/Top-N、上下文长度上限保持一致**，只改检索策略。

4. **比较谁更好**
   - **检索层面**：Top-K/Top-N 的证据“是否真的相关”（CUAD 可用标注 span 所在 chunk 来核对）。
   - **回答层面**：回答正确性、事实性、引用是否靠谱（Citation Precision/事实一致性等）。

### 详细 Plan（BioASQ-QA）

#### 第 1 步：下载并落盘 BioASQ-QA
- **你要做什么**：从 BioASQ 官方/镜像获取 QA 数据。
- **你要得到什么文件**：`data/bioasq/raw.json`（或等价原始格式）。

#### 第 2 步：把“可检索语料库”准备出来（这一步决定检索对比是否有意义）
你可以选一种方式（建议先用 A 跑通，再升级到 B）：
- **方式 A（最省事、能跑通）**：把 BioASQ 提供的 snippet/相关材料整理成语料。
- **方式 B（更像真实 RAG、效果更能拉开差距）**：用 PubMed 摘要/指南文本作为大语料库。

把语料切 chunk，输出一个 `jsonl` 文件（每行一个 chunk），字段至少包含：
- `doc_id`：文档 ID（如 PMID 或自定义 ID）
- `chunk_id`：chunk 序号
- `text`：chunk 文本
- `meta`：标题/年份/来源等（可选但建议保留）

**你要得到的文件**：`corpus/bioasq_chunks.jsonl`。

#### 第 3 步：建 4 套检索（对照组），并确保它们用的是同一个语料库
- **Sparse Only（BM25）**
  - **做什么**：对 `corpus/bioasq_chunks.jsonl` 建 BM25 索引
  - **输出**：`indexes/bioasq_bm25/`
- **Dense Only（向量检索）**
  - **做什么**：对每个 chunk 计算 embedding，建向量索引
  - **输出**：`indexes/bioasq_dense/`
- **Hybrid（混合检索）**
  - **做什么**：对同一 query 同时拿 BM25 分与向量分，归一化后按 \(\alpha\) 融合（默认 \(\alpha=0.7\)）
  - **输出**：`indexes/bioasq_hybrid/`（或保存融合配置与缓存结果）
- **Hybrid + Rerank（混合 + 重排）**
  - **做什么**：Hybrid 先召回 Top-K，再用 Cross-Encoder 对 (query, chunk) 重排
  - **输出**：`indexes/bioasq_rerank/`（或保存 rerank 模型/缓存）

#### 第 4 步：固定生成模型与提示词，跑端到端问答（4 套检索各跑一遍）
对每个问题 \(x\)：
1) 用当前检索策略召回 Top-K  
2) 取 Top-N（例如 N=3）拼成上下文  
3) 让 LLM 生成回答（要求带引用）  

每种方法单独写出预测文件：
- `runs/exp1/sparse_only/predictions.jsonl`
- `runs/exp1/dense_only/predictions.jsonl`
- `runs/exp1/hybrid/predictions.jsonl`
- `runs/exp1/hybrid_rerank/predictions.jsonl`

每行建议包含：`question_id, question, retrieved_chunks(topK), context(topN), answer, citations, latency_ms`。

#### 第 5 步：算指标（先做“最小可行”，再升级）
- **最小可行（强烈建议先做）**
  - **Citation Precision（抽样人工核验）**：随机抽样 100 条，检查“引用的 chunk 是否真的支持该句断言”。
  - **延迟/成本**：平均 latency、平均检索条数、rerank 次数。
- **升级版（更像论文）**
  - **检索质量**：Recall@K/MRR/nDCG@K（若能构造“相关证据”标签，例如使用 BioASQ 提供的 snippet/PMID 作为弱标签）
  - **回答质量**：FactScore 或人工事实一致性评分（同样可抽样）

#### 第 6 步：出图与写结论
- 4 种方法的指标柱状图
- 4 种方法的“效果 vs 成本”权衡图（rerank 往往更准但更贵） -->



---

## 实验二：语义熵 \(SE(x)\) 的幻觉/错误检测（高熵是否更容易错）

### 能用哪些数据集（推荐优先级）
- **强推荐**：**PubMedQA**（yes/no/maybe，天然“对/错”标签，易评测）

### 分别怎么用（通俗步骤）
核心思想：对每题让模型“多答几次”，看答案语义是否分裂，分裂越严重（熵越高）越可能错。

1. **对每个问题采样 \(M\) 次答案**
   - 同一题让模型生成 **\(M=10\)** 个候选（保持温度/采样策略固定）。

2. **用语义聚类算语义熵 \(SE(x)\)**
   - 通过 NLI 的双向蕴含近似判断语义等价 → 贪婪聚类 → 得到语义类分布 → 计算香农熵。

3. **给每题一个“对/错”标签（尽量自动化）**
   - PubMedQA：模型输出 yes/no/maybe 是否等于标准标签。

4. **检验“熵是否能预警错误”**
   - 通俗判断：**熵高的题是否更容易答错？**
   - 论文指标：**Spearman 相关系数**、**Pearson 相关系数**（\(SE(x)\) 与错误程度/错误标签的相关性）。

### 详细 Plan（PubMedQA）

#### 第 1 步：下载并落盘 PubMedQA
- **你要得到什么文件**：`data/pubmedqa/raw.json`（或原始格式）。

#### 第 2 步：把“标准标签”和“模型输出格式”统一成三分类
- PubMedQA 的 gold label 是：`yes / no / maybe`
- **你要做什么**：强制模型输出一个明确的最终标签（例如 `Final: yes`），避免“长答案里含糊其辞”导致判分困难。
- **你要得到什么文件**：`data/pubmedqa/gold_labels.jsonl`（question_id, gold_label）。

#### 第 3 步：对每题采样 \(M\) 次候选回答（用于估计语义熵）
- **你要做什么**：同一题用固定 temperature/top-p 生成 **\(M=10\)** 份候选；每份都规整成 `Final: yes/no/maybe`（可以附理由）。
- **你要得到什么文件**：`runs/exp2/samples.jsonl`（每题包含 10 个候选）。

#### 第 4 步：用 NLI 做“双向蕴含”判定并聚类（得到语义等价类）
- **你要做什么**
  - 对候选回答做语义等价判断：\(s_i \Leftrightarrow s_j\)（双向蕴含都成立）
  - 用贪婪聚类把 M 个候选分到若干语义类 \(c_1..c_K\)
- **你要得到什么文件**：`runs/exp2/clusters.jsonl`（每题：类划分、代表答案等）。

#### 第 5 步：计算语义熵 \(SE(x)\)
你有两种计算路径（论文里要写清楚用哪种）：
- **路径 A（更标准）**：用模型 logprob 估计每个候选 \(p(s|x)\)，再汇总到 \(p(c_k|x)\) 算熵。

**你要得到什么文件**：`runs/exp2/semantic_entropy.jsonl`（question_id, SE(x)）。

#### 第 6 步：给每题判对/错，并用相关系数评估“熵能否预警错误”
- **对/错标签**：模型最终预测标签（或多数票标签）是否等于 gold label，并转成数值变量（例如：错=1、对=0）。
- **评估方式（按你的要求）**
  - **Spearman**：计算 \(\rho(SE(x),\;error)\)（秩相关，更稳健）
  - **Pearson**：计算 \(r(SE(x),\;error)\)（线性相关）
  - 建议同时报告 **相关系数 + p-value**（便于论文表述显著性）
- **你要得到什么文件**：`runs/exp2/correlation.json`（Spearman/Pearson 系数与 p-value）+ `plots/exp2_scatter.png`（可选）

#### 第 7 步（建议）：做“熵分桶 vs 错误率”图（最通俗直观）
- 把样本按 \(SE(x)\) 从低到高分成 5～10 个桶
- 画每个桶的错误率折线图：通常会看到“熵越高，错得越多” 





<!-- ---

## 实验三：自反思机制的修正能力（Naive RAG vs Self-RAG）

### 能用哪些数据集（推荐优先级）
- **强推荐**：**BioASQ-QA**（证据链明显，适合测引用/支撑）

### 分别怎么用（通俗步骤）
核心思想：同样的证据文档下，对比“有无反思/批评机制”，看能否把错改对、或在无证据时变得更保守不瞎编。

1. **统一检索得到证据文档**
   - 为保证公平，尽量让不同系统拿到“同样的证据集合”（或用同一检索策略）。

2. **跑 3 个对照系统**
   - Naive RAG：检索后直接生成。
   - Self-RAG（去反思权重）：有 critic/反思输出，但不让其影响选择。
   - Self-RAG（完整）：用相关性/支持度/有用性反思信号参与片段选择/重写。

3. **看是否发生“纠错/保守输出”**
   - **Citation Precision**：引用的句子/段落是否真的能在证据里找到支撑。
   - **FactScore/事实一致性**：回答中的事实点是否被证据支持。
   - **纠错增益**：反思前 vs 反思后，事实性提升了多少；以及是否引入新错误。
   - **拒答/截断率**：证据不足时是否停止编造（更安全但可能更保守）。

### 详细 Plan（BioASQ-QA）

#### 第 1 步：复用实验一的语料与检索（把变量控制住）
- **你要做什么**：直接复用 `corpus/bioasq_chunks.jsonl` 与实验一的检索策略/索引。
- **为什么**：实验三要比较“反思机制”，不希望检索差异掺进来。

#### 第 2 步：实现 3 个对照系统，并保证它们看到的证据一致
- **Naive RAG**：检索 Top-N → 直接生成（带引用）
- **Self-RAG（去反思权重）**：输出反思标签，但不参与选择（等价于只用生成概率）
- **Self-RAG（完整）**：用反思信号参与片段选择/重写（你们的 Score 融合思路）

把三套系统的输出分别保存：
- `runs/exp3/naive_rag/predictions.jsonl`
- `runs/exp3/self_rag_wo_weight/predictions.jsonl`
- `runs/exp3/self_rag_full/predictions.jsonl`

#### 第 3 步：把“Positive 的定义”写死（否则不可复现）
- 相关性：`Rel` 视为 Positive
- 支持度：`Fully` 视为 Positive（或 `Fully+Partial`，二选一并写清）
- 有用性：分数 ≥ 4 视为 Positive（阈值固定）

#### 第 4 步：评估“它到底有没有纠错”（建议同时看收益与副作用）
- **Citation Precision**
  - 通俗做法：抽样核对“引用的 chunk 里是否真的能找到支撑句/关键事实”。
- **FactScore / 事实一致性（抽样人工）**
  - 通俗做法：把回答拆成 2–5 条事实点（claim），逐条看证据是否支持。
- **纠错增益（必做）**
  - 统计：错误修正率（错→对）、错误引入率（对→错）、\(\Delta\)FactScore（full - naive）。
- **保守性（建议）**
  - 拒答/截断率：在证据不足时是否停止编造（更安全，但可能降低覆盖率）。

#### 第 5 步：结果展示（建议论文写法）
- 三个系统的指标对比表（Citation Precision / FactScore / 拒答率 / 成本）
- “纠错成功 vs 引入新错”的对比（读者一眼就懂）

--- --> -->


