# 实验与数据集匹配指南（对应 `实验设计整理.md` 的三个实验）

本文件整理了 `rag场景与数据集.pdf` 中给出的数据集。

---

## 可执行实验 Plan（按步骤照做）

下面给你一份**可以照着一步步做**的实验执行计划（对应 `实验设计整理.md` 的实验一/二/三）。

- **你当前选择的数据集（按本文推荐）**
  - 实验一：**BioASQ-QA**
  - 实验二：**PubMedQA**
  - 实验三：**BioASQ-QA**

> 如果你改走法律路线（CUAD / CaseHOLD），整体步骤完全一样；你只需要把“语料库怎么建、指标怎么判定”那两处替换掉即可。

### 0) 通用准备（建议先做完再跑三个实验）

#### 0.1 建议的文件夹结构（为了不乱）
- `data/`：原始数据、清洗后的数据、划分后的 train/dev/test
- `corpus/`：可检索语料库（chunk 后的文档）
- `indexes/`：BM25 索引、向量索引、rerank 模型缓存（如有）
- `runs/`：每次实验的输出（检索结果、回答、引用、日志、指标）
- `plots/`：相关散点图/分桶图、柱状图、成本-效果曲线（如有 ROC/PR 也放这里）

#### 0.2 固定“对比必须一致”的参数（写进实验设置，保证公平）
- **Chunk**：chunk size（300 tokens）与 overlap（30 tokens）固定
- **检索**：Top-K（召回数，K=20）固定；Top-N（注入上下文数，N=3）固定
- **生成**：同一 LLM、同一提示词模板、同一解码参数（temperature/top-p/max_tokens）固定
- **实验二采样**：固定 **\(M=10\)**；后续可再做 5/10/20 的敏感性（可选）

#### 0.3 统一“引用输出格式”（否则实验三很难评）
让模型在回答中对关键断言给引用：
- 在句末加 `[doc_id:chunk_id]`


<!-- 
## 实验一：动态检索策略有效性（Sparse vs Dense vs Hybrid vs Rerank）

### 能用哪些数据集（推荐优先级）
- **强推荐**：**BioASQ-QA**（多文档证据天然适配）

### 分别怎么用（通俗步骤）
以 **BioASQ-QA**为例（最像“真正的 RAG 实验”）：

1. **先建一个“可检索语料库”**
   - BioASQ：把医学文献摘要/题目相关材料切分成 chunk 建索引。

2. **同一批问题，分别跑 4 种检索策略**
   - Sparse Only：BM25
   - Dense Only：向量检索
   - Hybrid：BM25 + 向量加权融合
   - Hybrid + Rerank：Hybrid 召回后，再用 Cross-Encoder 重排序

3. **把 Top-N 文档作为上下文，喂给同一个生成模型回答**
   - 关键是：**生成模型、提示词、Top-K/Top-N、上下文长度上限保持一致**，只改检索策略。

4. **比较谁更好**
   - **检索层面**：Top-K/Top-N 的证据“是否真的相关”（CUAD 可用标注 span 所在 chunk 来核对）。
   - **回答层面**：回答正确性、事实性、引用是否靠谱（Citation Precision/事实一致性等）。

### 详细 Plan（BioASQ-QA）

#### 第 1 步：下载并落盘 BioASQ-QA
- **你要做什么**：从 BioASQ 官方/镜像获取 QA 数据。
- **你要得到什么文件**：`data/bioasq/raw.json`（或等价原始格式）。

#### 第 2 步：把“可检索语料库”准备出来（这一步决定检索对比是否有意义）用 PubMed 摘要/指南文本作为大语料库。

把语料切 chunk，输出一个 `jsonl` 文件（每行一个 chunk），字段至少包含：
- `doc_id`：文档 ID（如 PMID 或自定义 ID）
- `chunk_id`：chunk 序号
- `text`：chunk 文本
- `meta`：标题/年份/来源等（可选但建议保留）

**你要得到的文件**：`corpus/bioasq_chunks.jsonl`。

#### 第 3 步：建 4 套检索（对照组），并确保它们用的是同一个语料库
- **Sparse Only（BM25）**
  - **做什么**：对 `corpus/bioasq_chunks.jsonl` 建 BM25 索引
  - **输出**：`indexes/bioasq_bm25/`
- **Dense Only（向量检索）**
  - **做什么**：对每个 chunk 计算 embedding，建向量索引
  - **输出**：`indexes/bioasq_dense/`
- **Hybrid（混合检索）**
  - **做什么**：对同一 query 同时拿 BM25 分与向量分，归一化后按 \(\alpha\) 融合（默认 \(\alpha=0.7\)）
  - **输出**：`indexes/bioasq_hybrid/`（或保存融合配置与缓存结果）
- **Hybrid + Rerank（混合 + 重排）**
  - **做什么**：Hybrid 先召回 Top-K，再用 Cross-Encoder 对 (query, chunk) 重排
  - **输出**：`indexes/bioasq_rerank/`（或保存 rerank 模型/缓存）

#### 第 4 步：固定生成模型与提示词，跑端到端问答（4 套检索各跑一遍）
对每个问题 \(x\)：
1) 用当前检索策略召回 Top-K  
2) 取 Top-N（例如 N=3）拼成上下文  
3) 让 LLM 生成回答（要求带引用）  

每种方法单独写出预测文件：
- `runs/exp1/sparse_only/predictions.jsonl`
- `runs/exp1/dense_only/predictions.jsonl`
- `runs/exp1/hybrid/predictions.jsonl`
- `runs/exp1/hybrid_rerank/predictions.jsonl`

每行建议包含：`question_id, question, retrieved_chunks(topK), context(topN), answer, citations, latency_ms`。

#### 第 5 步：算指标（先做“最小可行”，再升级）
- **最小可行（强烈建议先做）**
  - **Citation Precision（抽样人工核验）**：随机抽样 100 条，检查“引用的 chunk 是否真的支持该句断言”。
  - **延迟/成本**：平均 latency、平均检索条数、rerank 次数。
- **升级版（更像论文）**
  - **检索质量**：Recall@K/MRR/nDCG@K（若能构造“相关证据”标签，例如使用 BioASQ 提供的 snippet/PMID 作为弱标签）
  - **回答质量**：FactScore 或人工事实一致性评分（同样可抽样）

#### 第 6 步：出图与写结论
- 4 种方法的指标柱状图
- 4 种方法的“效果 vs 成本”权衡图（rerank 往往更准但更贵）

#### 一键跑完实验一（代码实现）
- 预处理脚本：`scripts/exp1_prepare_bioasq_data.py`
  - 从 `data/BioASQ/trainining14b.json` 提取问题与 snippets，生成：
    - `corpus/bioasq_chunks.jsonl`
    - `data/bioasq/qa.jsonl`
- 检索与问答脚本：`scripts/exp1_run_retrieval_qa.py`
  - 支持四种策略：`sparse_only,dense_only,hybrid,hybrid_rerank`
  - 输出：
    - `runs/exp1/sparse_only/predictions.jsonl`
    - `runs/exp1/dense_only/predictions.jsonl`
    - `runs/exp1/hybrid/predictions.jsonl`
    - `runs/exp1/hybrid_rerank/predictions.jsonl`
- 评估与可视化脚本：`scripts/exp1_evaluate_and_plot.py`
  - 输出：
    - `runs/exp1/metrics_summary.json`
    - `plots/exp1_retrieval_metrics.png`（单图）
    - `plots/exp1_effect_vs_cost.png`（单图）
- 一键流水线：`scripts/exp1_run_pipeline.py`
  - 快速冒烟（只跑 100 题，先检查流程）：
    - `python scripts/exp1_run_pipeline.py --llm_model <你的模型名> --max_questions 100 --top_k 20 --top_n 3 --workers 4`
  - 全量运行：
    - `python scripts/exp1_run_pipeline.py --llm_model <你的模型名> --top_k 20 --top_n 3 --workers 4`
  - 若暂时只做检索评估（不生成回答）：
    - `python scripts/exp1_run_pipeline.py --skip_generation --max_questions 500 --workers 4` -->



<!-- ---

## 实验二：语义熵 \(SE(x)\) 的幻觉/错误检测（高熵是否更容易错）

### 能用哪些数据集（推荐优先级）
- **强推荐**：**PubMedQA**（yes/no/maybe，天然“对/错”标签，易评测）

### 分别怎么用（通俗步骤）
核心思想：对每题让模型“多答几次”，看答案语义是否分裂，分裂越严重（熵越高）越可能错。

1. **对每个问题采样 \(M\) 次答案**
   - 同一题让模型生成 **\(M=10\)** 个候选（保持温度/采样策略固定）。
   - 每个候选都应包含**可比较的语义内容**（简短解释/证据/推理），并在末尾给出一个可判分的三分类标签：`Final: yes/no/maybe`。

2. **用语义聚类算语义熵 \(SE(x)\)**
   - 通过 NLI 的双向蕴含近似判断语义等价 → 贪婪聚类 → 得到语义类分布 → 计算香农熵。
   - 注意：这里的“语义等价/聚类”应主要基于**解释/证据文本**（而不是仅基于 `Final:` 标签），否则会退化成 `yes/no/maybe` 三桶计数，失去语义熵的意义。

3. **给每题一个“对/错”标签（尽量自动化）**
   - PubMedQA：模型输出 yes/no/maybe 是否等于标准标签。

4. **检验“熵是否能预警错误”**
   - 通俗判断：**熵高的题是否更容易答错？**
   - 论文指标：**Spearman 相关系数**、**Pearson 相关系数**（\(SE(x)\) 与错误程度/错误标签的相关性）。

### 详细 Plan（PubMedQA）

#### 第 1 步：下载并落盘 PubMedQA
- **你要得到什么文件**：`data/pubmedqa/raw.json`（或原始格式）。

#### （建议）你用什么模型？如何调用模型？
- **模型要求（最低）**：能按 `temperature/top-p` 做随机采样，并且能稳定遵守你规定的输出格式（含 `Answer:` 与 `Final:` 两行）。
- **推荐做法（最省心、最容易复现）**：使用 **OpenAI-compatible 的 Chat API** 调用任意你能访问的模型（OpenAI / Azure OpenAI / 各类兼容接口的国产模型等）。这样后续脚本基本不改就能切模型。
- **调用方式（本仓库给的最小可跑脚本）**：见 `scripts/exp2_sample_pubmedqa_openai.py`。
  - 安装依赖：`pip install -r requirements.txt`
  - 配置 Key：设置环境变量 `OPENAI_API_KEY`（如果你用兼容接口，也可以额外设置 `OPENAI_BASE_URL`）
  - 运行采样：`python scripts/exp2_sample_pubmedqa_openai.py --input data/pubmedqa/ori_pqal.json --out runs/exp2/samples.jsonl --model <你的模型名> --m 10`
  - 本地模型（如 LM Studio/Ollama）可直接复用：把 `OPENAI_BASE_URL` 指向本地服务地址（如 `http://localhost:1234/v1`）即可。

#### 一键跑完实验二（推荐）
- 脚本：`scripts/exp2_run_pipeline.py`
- 作用：自动串联 `gold labels -> M 次采样 -> 语义聚类/语义熵 -> 相关性 -> 可视化`
- 版本化运行（推荐）：用 `--run_tag v2` 让输出自动写到 `samples_v2 / clusters_v2 / semantic_entropy_v2 / correlation_v2 / 图文件_v2`，不覆盖已有结果。
- 默认语义熵实现：`nli_llm`（双向蕴含 + 贪婪聚类，贴近论文描述）；可用 `--semantic_backend tfidf` 退回近似快速版
- 示例：
  - 快速冒烟（先跑 30 题）：
    - `python scripts/exp2_run_pipeline.py --model <你的模型名> --m 10 --max_items 30 --require_same_label --resume --semantic_backend nli_llm`
  - 全量运行：
    - `python scripts/exp2_run_pipeline.py --model <你的模型名> --m 10 --require_same_label --resume --semantic_backend nli_llm`
  - 新策略复现实验（不覆盖旧结果）：
    - `python scripts/exp2_run_pipeline.py --model <你的模型名> --m 10 --workers 4 --max_context_chars 2500 --max_tokens 160 --concise_mode --append_no_think --require_same_label --semantic_backend nli_llm --prediction_mode maybe_debiased --plot_error_metric ordinal --nli_workers 4 --run_tag v2`
- 提速配置（目标 5～6 小时，保持 `m=10`）：
  - `python scripts/exp2_run_pipeline.py --model <你的模型名> --m 10 --workers 4 --max_context_chars 2500 --max_tokens 160 --concise_mode --append_no_think --require_same_label --resume --semantic_backend nli_llm --prediction_mode maybe_debiased --plot_error_metric ordinal`
  - 说明：通过并发采样 + 上下文裁剪 + 适度精简理由输出提速，不依赖减少 `m`，尽量保留语义聚类有效性与回答可读性。
- 主要输出文件：
  - `data/pubmedqa/gold_labels.jsonl`
  - `runs/exp2/samples.jsonl`
  - `runs/exp2/clusters.jsonl`
  - `runs/exp2/semantic_entropy.jsonl`
  - `runs/exp2/correlation.json`
  - `plots/exp2_entropy_scatter.png`
  - `plots/exp2_entropy_bucket_bar.png`
- 可视化风格说明（默认实现）：
  - 单图单画布（每个 PNG 只放 1 个主图，不做多子图拼接）
  - 采用低饱和蓝灰配色 + 衬线字体，适配论文风格（简洁、稳重）

#### 第 2 步：把“标准标签”和“模型输出格式”统一成三分类
- PubMedQA 的 gold label 是：`yes / no / maybe`
- **你要做什么**：强制模型输出一个明确的最终标签（例如 `Final: yes`），避免“长答案里含糊其辞”导致判分困难。
- **同时要强调**：最终标签只用于**自动判分/对错标注**；语义熵的聚类阶段仍需要候选答案里保留可比较的解释/证据文本。
- **你要得到什么文件**：`data/pubmedqa/gold_labels.jsonl`（question_id, gold_label）。

#### 第 3 步：对每题采样 \(M\) 次候选回答（用于估计语义熵）
- **你要做什么**：同一题用固定 temperature/top-p 生成 **\(M=10\)** 份候选；每份都要求输出“解释/证据 + 最终标签”。
  - 推荐强约束输出格式（便于后续抽取与聚类）：
    - `Answer:` 一段简短解释/证据（2～6 句，尽量引用题干信息或医学常识依据）
    - `Final: yes|no|maybe`（严格三选一）
  - 后处理时，把候选答案拆成两个字段：
    - `rationale_text`：用于 NLI/聚类的解释文本（通常是 `Answer:` 部分）
    - `final_label`：用于自动判分的三分类标签（`Final:` 部分）
- **你要得到什么文件**：`runs/exp2/samples.jsonl`（每题包含 10 个候选；每个候选至少含 `sample_id, rationale_text, final_label`）。

#### 第 4 步：用 NLI 做“双向蕴含”判定并聚类（得到语义等价类）
- **你要做什么**
  - 对候选回答做语义等价判断：\(s_i \Leftrightarrow s_j\)（双向蕴含都成立）
  - 这里的 \(s_i\) 建议取 `rationale_text`（解释/证据文本），而不是只取 `final_label`
  - 用贪婪聚类把 M 个候选分到若干语义类 \(c_1..c_K\)，再由每类的样本数/概率得到 \(p(c_k|x)\)
  - 工程实现建议：先用候选与各类代表答案做双向蕴含判定；若与某类代表互相蕴含则并入该类，否则新建一类
- **你要得到什么文件**：`runs/exp2/clusters.jsonl`（每题：类划分、代表答案等）。

#### 第 5 步：计算语义熵 \(SE(x)\)
- **路径 A（更标准）**：用模型 logprob 估计每个候选 \(p(s|x)\)，再汇总到 \(p(c_k|x)\) 算熵。

**你要得到什么文件**：`runs/exp2/semantic_entropy.jsonl`（question_id, SE(x)）。

#### 第 6 步：给每题判对/错，并用相关系数评估“熵能否预警错误”
- **对/错标签（建议升级）**
  - `binary error`：预测标签是否等于 gold（错=1、对=0）
  - `ordinal error`：把 `no/maybe/yes` 映射到 `-1/0/1`，按距离计算错误程度（更细粒度）
  - `maybe_overuse`：预测为 maybe 且 gold 为 yes/no 的比例（用于诊断 maybe 滥用）
- **预测标签（建议）**：优先使用 `maybe_debiased`（多数票基础上对“边界型 maybe”做轻量去偏），并同时报告原始多数票分布
- **评估方式（按你的要求）**
  - **Spearman**：计算 \(\rho(SE(x),\;error)\)（秩相关，更稳健）
  - **Pearson**：计算 \(r(SE(x),\;error)\)（线性相关）
  - 建议同时报告 **相关系数 + p-value**（便于论文表述显著性）
- **你要得到什么文件**：`runs/exp2/correlation.json`（Spearman/Pearson 系数与 p-value）+ `plots/exp2_entropy_scatter.png`（可选）

#### 第 7 步（建议）：做“熵分桶 vs 错误率”图（最通俗直观）
- 把样本按 \(SE(x)\) 从低到高分成 5～10 个桶
- 画每个桶的错误率折线图：通常会看到“熵越高，错得越多”  -->





---

## 实验三：自反思机制的修正能力（Naive RAG vs Self-RAG）

### 能用哪些数据集（推荐优先级）
- **强推荐**：**BioASQ-QA**（证据链明显，适合测引用/支撑）

### 分别怎么用（通俗步骤）
核心思想：同样的证据文档下，对比“有无反思/批评机制”，看能否把错改对、或在无证据时变得更保守不瞎编。

1. **统一检索得到证据文档**
   - 为保证公平，尽量让不同系统拿到“同样的证据集合”（或用同一检索策略）。

2. **跑 3 个对照系统**
   - Naive RAG：检索后直接生成。
   - Self-RAG（去反思权重）：有 critic/反思输出，但不让其影响选择。
   - Self-RAG（完整）：用相关性/支持度/有用性反思信号参与片段选择/重写。

3. **看是否发生“纠错/保守输出”**
   - **Citation Precision**：引用的句子/段落是否真的能在证据里找到支撑。
   - **FactScore/事实一致性**：回答中的事实点是否被证据支持。
   - **纠错增益**：反思前 vs 反思后，事实性提升了多少；以及是否引入新错误。
   - **拒答/截断率**：证据不足时是否停止编造（更安全但可能更保守）。

### 详细 Plan（BioASQ-QA）

#### 第 1 步：复用实验一的语料与检索（把变量控制住）
- **你要做什么**：直接复用 `corpus/bioasq_chunks.jsonl` 与实验一的检索策略/索引。
- **为什么**：实验三要比较“反思机制”，不希望检索差异掺进来。

#### 第 2 步：实现 3 个对照系统，并保证它们看到的证据一致
- **Naive RAG**：检索 Top-N → 直接生成（带引用）
- **Self-RAG（去反思权重）**：输出反思标签，但不参与选择（等价于只用生成概率）
- **Self-RAG（完整）**：用反思信号参与片段选择/重写（你们的 Score 融合思路）

把三套系统的输出分别保存：
- `runs/exp3/naive_rag/predictions.jsonl`
- `runs/exp3/self_rag_wo_weight/predictions.jsonl`
- `runs/exp3/self_rag_full/predictions.jsonl`

#### 第 3 步：把“Positive 的定义”写死（否则不可复现）
- 相关性：`Rel` 视为 Positive
- 支持度：`Fully` 视为 Positive
- 有用性：分数 ≥ 4 视为 Positive（阈值固定）

#### 第 4 步：评估“它到底有没有纠错”（建议同时看收益与副作用）
- **Citation Precision**
  - 通俗做法：抽样核对“引用的 chunk 里是否真的能找到支撑句/关键事实”。
- **FactScore / 事实一致性（抽样人工）**
  - 通俗做法：把回答拆成 2–5 条事实点（claim），逐条看证据是否支持。
- **纠错增益（必做）**
  - 统计：错误修正率（错→对）、错误引入率（对→错）、\(\Delta\)FactScore（full - naive）。
- **保守性（建议）**
  - 拒答/截断率：在证据不足时是否停止编造（更安全，但可能降低覆盖率）。

#### 第 5 步：结果展示（建议论文写法）
- 三个系统的指标对比表（Citation Precision / FactScore / 拒答率 / 成本）
- “纠错成功 vs 引入新错”的对比（读者一眼就懂）

#### 一键跑完实验三（代码实现）
- 核心脚本：`scripts/exp3_run_selfrag.py`
  - 输入：`runs/exp1/hybrid_rerank/predictions.jsonl`（默认）
  - 三个系统输出：
    - `runs/exp3/naive_rag/predictions.jsonl`
    - `runs/exp3/self_rag_wo_weight/predictions.jsonl`
    - `runs/exp3/self_rag_full/predictions.jsonl`
  - 实现要点（与公式对齐）：
    - 反思维度：`rel / sup / use`
    - 评分函数：`Score = P(y|x,d) + theta * (w_rel*P(rel=Pos) + w_sup*P(sup=Pos) + w_use*P(use=Pos))`
    - `self_rag_wo_weight`：将 `w_rel=w_sup=w_use=0`（去反思权重）
- 评估与可视化：`scripts/exp3_evaluate_and_plot.py`
  - 输出：
    - `runs/exp3/metrics_summary.json`
    - `plots/exp3_quality_metrics.png`（单图）
    - `plots/exp3_correction_gain.png`（单图）
- 一键流水线：`scripts/exp3_run_pipeline.py`
  - 快速冒烟：
    - `python scripts/exp3_run_pipeline.py --model <你的模型名> --max_questions 100 --num_candidates 3 --w_sup 2.0`
  - 全量运行：
    - `python scripts/exp3_run_pipeline.py --model <你的模型名> --num_candidates 3 --w_sup 2.0`

#### 跑完实验一后自动接实验三
- 串联入口：`scripts/exp1_then_exp3_pipeline.py`
- 用法：
  - `python scripts/exp1_then_exp3_pipeline.py --llm_model <你的模型名> --max_questions 200 --top_k 20 --top_n 3 --exp1_workers 4 --exp3_num_candidates 3 --exp3_w_sup 2.0`
- 作用：先执行实验一完整流程，再自动使用 `runs/exp1/hybrid_rerank/predictions.jsonl` 作为实验三输入继续运行。

--- -->


