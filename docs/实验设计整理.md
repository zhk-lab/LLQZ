### 实验设计整理（阅读 `实验设计指导.pdf` 后改进版）

本文件将原 PDF 中的实验按 **实验目的 / 实验方法 / 评价指标** 三个维度重新整理，并补齐可复现的设置、对照组与建议的消融实验，便于直接写入论文或项目文档。

---

### 统一实验设置（建议写在“实验设置”章节）

- **任务与数据**：选用你的垂直领域测试集（每条样本至少包含：问题 \(x\)、可检索语料库/知识库、参考答案或事实核验口径）。如无标准答案，至少需要“事实正确/错误”的人工或规则标注用于幻觉评测。
- **生成模型**：同一主模型、同一提示词模板；固定解码参数（temperature、top-p、max tokens 等），确保对比公平。
- **检索语料**：统一分块（chunk size/overlap）、统一索引与过滤规则（去重、时间范围、权限等）。
- **关键超参数（来自 PDF，可作为默认值）**：
  - **语义熵阈值** \(\tau\)：默认 \(\tau=0.5\)，当 \(SE(x)>\tau\) 判定为高不确定性/高幻觉风险，触发检索或更强纠错流程。
  - **混合检索权重** \(\alpha\)：默认 \(\alpha=0.7\)（稠密 70% + 稀疏 BM25 30%）。
  - **反思维度权重**：默认提高“支持度”权重（例如 \(w_{sup}=2.0\)）以优先保证事实准确性。

---

### 实验一：动态检索策略的有效性验证（检索层面 + 下游问答）

- **实验目的**
  - 验证提出的 **混合检索（BM25 + 向量）** 与 **Cross-Encoder 重排序** 是否优于单一检索方式，并量化其对最终回答质量（事实性/可引用性）的提升。

- **实验方法**
  - **对照组（来自 PDF）**：
    - **Sparse Only**：仅 BM25（稀疏检索）
    - **Dense Only**：仅向量检索（稠密检索）
    - **Hybrid (Ours)**：混合检索（加权融合）
    - **Hybrid + Rerank (Ours Pro)**：混合检索 + Cross-Encoder 重排序
  - **控制变量**：
    - 统一 Top-K 召回数、统一最终注入上下文的文档数 Top-N（例如 N=3）、统一生成模型与提示词。
  - **执行流程**：
    - 对每个问题 \(x\)，在不同检索策略下检索得到文档集合 \(D\)，将 Top-N 文档拼接为上下文输入生成器，产出回答 \(y\) 与引用信息（如有）。

- **评价指标（建议至少覆盖“检索质量 + 回答质量 + 代价”）**
  - **检索质量**：Recall@K、MRR、nDCG@K（若有“相关文档”标注）；或采用人工抽检 Top-K 相关率。
  - **回答质量**：FactScore（事实性）、Hallucination Rate（幻觉率，需标注口径）、Answer F1/EM（如有标准答案）。
  - **引用质量**：Citation Precision（引用准确率：被引用句子是否能被检索文档支撑）。


---

### 实验二：基于语义熵的幻觉检测验证（不确定性度量是否有效）

- **实验目的**
  - 验证定义的 **语义熵 \(SE(x)\)** 能否有效区分“正确回答”与“幻觉回答”，并作为触发检索/截断的可靠信号。

- **实验方法**
  - 对每个问题 \(x\) 进行 **\(M\) 次采样**（固定温度与采样策略），得到候选回答集合 \(S=\{s_1,\dots,s_M\}\)。
  - 使用 **双向蕴含判定（NLI 近似）** + **贪婪语义聚类** 将候选回答划分为语义等价类集合 \(C=\{c_1,\dots,c_K\}\)。
  - 计算各语义类概率 \(p(c_k|x)\approx \sum_{s\in c_k}p(s|x)\)，并计算语义熵：
    \[
    SE(x) = -\sum_{k=1}^{K} p(c_k|x)\log p(c_k|x)
    \]
  - 将 \(SE(x)\) 与“事实错误/幻觉标注”做相关性与分类效果评测；并以阈值 \(\tau\)（默认 0.5）做触发判定：\(SE(x)>\tau\) 视为高风险。
  - **改进建议（更像论文实验）**：
    - 增加不确定性基线：token 熵 / 平均负对数似然（NLL）/ 模型自评置信（自评打分）等，与语义熵做对比。

- **评价指标**
  - **二分类检测能力**：AUROC、AUPRC、F1（在阈值 \(\tau\) 下）、Accuracy（可选）。
  - **相关性分析**：Spearman/Pearson 相关系数（\(SE(x)\) 与幻觉率/事实错误率）。
---

### 实验三：自反思机制的修正能力验证（检索-生成-批评是否能纠错）

- **实验目的**
  - 验证 **检索-生成-批评（Critic/反思令牌）** 流程能否在出现错误或支撑不足时实现自动修正（或更保守输出/停止编造），提升事实性与可引用性。

- **实验方法**
  - **对照组（来自 PDF，建议保持命名一致）**：
    - **Naive RAG**：无反思机制，检索后直接生成。
    - **Self-RAG (w/o reflection weight)**：有检索，但移除反思令牌权重（例如令 \(w_k=0\)），只依赖基础生成概率。
    - **Self-RAG (full)**：完整模型，启用相关性/支持度/有用性反思令牌与加权评分函数进行片段选择/重写。
  - **改进建议（把“修正能力”做成可量化的实验）**：
    - 设定“需要修正”的样本集：如包含易错事实点的问题、或故意加入干扰文档/缺证据文档的场景，观察系统是否会改写或保守输出。
    - 记录每个样本的“首次输出”与“反思后输出”，评估是否发生纠错、是否引入新错误。
    - 记录触发行为：是否触发二次检索、是否触发截断/拒答、平均迭代轮数（如有）。

- **评价指标（来自 PDF + 建议补充）**
  - **Citation Precision**：引用准确率（生成句子是否真的被检索文档支撑）。
  - **FactScore**：事实得分（或等价的事实一致性指标/人工标注准确率）。
  - **纠错增益**：\(\Delta\)FactScore（反思后 - 反思前）、错误修正率（原错→改对比例）、错误引入率（原对→改错比例）。
  - **安全性/保守性**：拒答/截断率（在证据不足时是否停止编造）。
  - **代价指标**：平均检索次数、平均生成轮数、端到端延迟。

---

### （新增建议）实验五：关键超参数消融（解释“为什么有效”）

- **实验目的**
  - 分析系统性能对关键参数的敏感性，增强可解释性与可复现性。

- **实验方法（示例）**
  - \(\tau\)：0.3 / 0.5 / 0.7（检索触发强度）
  - \(\alpha\)：0.3 / 0.5 / 0.7 / 0.9（稠密/稀疏检索占比）
  - \(w_{sup}\)：1.0 / 2.0 / 3.0（事实支撑优先级）
  - \(M\)：5 / 10 / 20（语义熵估计稳定性 vs 成本）

- **评价指标**
  - 与主实验一致（FactScore、Hallucination Rate、Citation Precision、延迟/成本），并输出“性能-成本”曲线用于选型。

